{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, spacy, codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"/Users/eunjilee/Desktop/cesta/short_stories_project/short_stories_files\"\n",
    "ofn = \"/Users/eunjilee/Desktop/cesta/short_stories_project/outputs/word_counts_output.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counting (for whole corpus and individual stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a language\n",
    "\n",
    "# original_language = \"ko\"\n",
    "original_language = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ko09.txt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'original_language' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-4b0c8653bfd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'original_language' is not defined"
     ]
    }
   ],
   "source": [
    "# wordcount\n",
    "\n",
    "total_counter = 0\n",
    "\n",
    "with open(ofn, 'w') as output:\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.startswith(original_language):\n",
    "            \n",
    "            filename = dir_name + \"/\" + filename\n",
    "            f = open(filename, 'r')\n",
    "            text = f.read()\n",
    "            \n",
    "            words = 0\n",
    "            for wordcount in text.split(\" \"):\n",
    "                words += 1\n",
    "            \n",
    "            output_string = filename + \": \" + str(words)\n",
    "            print(output_string)\n",
    "            output.write(output_string)\n",
    "            \n",
    "            total_counter += words\n",
    "    print(total_counter)\n",
    "    output.write(\"Total number of words: \" + str(total_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Length Counter\n",
    "## TODO: Generalize (not just for English and Korean), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "from spacy.lang.en import English\n",
    "import os, codecs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"/Users/eunjilee/Desktop/cesta/short_stories_project/short_stories_files\"\n",
    "\n",
    "output_file_en = \"/Users/eunjilee/Desktop/cesta/short_stories_project/outputs/avg_len_en\"\n",
    "output_file_ko = \"/Users/eunjilee/Desktop/cesta/short_stories_project/outputs/avg_len_ko\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a directory into a list of its text files\n",
    "\n",
    "def dir2files(somedirectory,path=False, extension = '.csv'):\n",
    "    if path == False:\n",
    "        files = os.listdir(somedirectory)\n",
    "    else:\n",
    "        files = os.listdir(somedirectory)\n",
    "        files = [os.path.join(somedirectory,f) for f in files]\n",
    "    for i in files[:]:\n",
    "            if not i.endswith(extension):\n",
    "                files.remove(i)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_2_sentence_list(raw_text):\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer')) \n",
    "    doc = nlp(raw_text)\n",
    "    \n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_2_sentence_lengths(filename, ofn):\n",
    "    reader = codecs.open(filename, encoding=\"utf-8-sig\")\n",
    "    text = reader.read()\n",
    "    reader.close()\n",
    "    \n",
    "    sentences_list = text_2_sentence_list(text)\n",
    "    len_list = []\n",
    "    \n",
    "    for sentence in sentences_list:\n",
    "        sentence_length = len(sentence.split()) \n",
    "        len_list.append(sentence_length)\n",
    "        '''\n",
    "        output_str = (filename[-8:], sentence, sentence_length)\n",
    "        sep = '\\t'\n",
    "        output_str = sep.join(output_str)\n",
    "        \n",
    "        print(sentence)\n",
    "        print(sentence_length)\n",
    "        '''\n",
    "        \n",
    "    average = float(sum(len_list)) / float(len(sentences_list))\n",
    "    stdev = np.std(len_list)\n",
    "    \n",
    "    output_str = (filename[-8:], str(average), str(stdev), '\\n')\n",
    "    sep = '\\t'\n",
    "    output_str = sep.join(output_str)\n",
    "    \n",
    "    with open(ofn, 'a') as output:\n",
    "        output.write(output_str)\n",
    "        output.close()\n",
    "        \n",
    "    return [average, stdev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.7073732718894, 8.760225941040988], [12.11875, 7.6912059156870844], [14.294332723948811, 13.934136822167144], [9.634188034188034, 7.258560079394646], [14.862385321100918, 12.414823848568178], [11.23936170212766, 10.98908921895165], [11.844444444444445, 7.8055693950055245], [9.872372372372372, 7.6975632480308285], [12.37730870712401, 7.950879019936119], [11.067415730337078, 9.884072709240373], [14.395778364116095, 9.680739145849373], [13.193343898573692, 12.469831235307188], [13.1425, 9.47772091538889], [7.95413870246085, 7.590551032306562], [21.462222222222223, 12.411809589417274], [14.216845878136201, 9.588744302131465], [10.004716981132075, 7.875204484622434], [13.16, 10.080176365300142], [21.459016393442624, 15.200920777214158], [35.421052631578945, 34.54361462228104], [10.316358024691358, 7.133228041467019], [13.118694362017804, 8.775683328372736], [12.732432432432432, 10.96018726546601], [12.62396694214876, 10.542864296250574], [10.065495207667732, 7.77194046687081], [12.16988416988417, 8.472650776675183], [11.797872340425531, 9.245274568991634], [9.068493150684931, 6.539299043168196], [16.205607476635514, 11.162971431165996], [9.939759036144578, 7.241279587367025], [18.761764705882353, 18.38059262159703], [16.138504155124654, 12.739568519879214], [9.936764705882354, 9.01243266127678], [21.756183745583037, 18.123270155389953], [16.67105263157895, 12.634596222237747], [8.71875, 5.79218185658239], [21.65814696485623, 22.552630792696714], [11.459537572254336, 7.793196515223321]]\n",
      "\n",
      "\n",
      "[[11.011513157894736, 7.409200295992993], [12.060532687651332, 8.380405874527371], [16.85641891891892, 11.971023263976607], [13.887351778656127, 9.477722897766355], [11.917979610750695, 7.183497280207355], [22.125, 17.76305271248911], [14.788732394366198, 6.972108402504549], [12.512084592145015, 9.025258557465152], [11.927404718693285, 7.014775201170322], [10.814016172506738, 6.560554985214602], [11.722650231124808, 6.732172948348497], [13.03448275862069, 8.09287994777174], [12.994400895856662, 7.760913062363663], [10.61614730878187, 6.790154503980881], [11.217054263565892, 7.321302224197371], [9.398477157360405, 5.6484217027158765], [16.954545454545453, 8.190777922469062], [11.482142857142858, 9.197226040319695], [11.796680497925312, 6.7218098433470725], [17.270833333333332, 11.393601243924397], [15.252347417840376, 9.245271936242627], [13.090196078431372, 8.979044337592683], [11.80921052631579, 8.706269332643046], [10.850299401197605, 6.860132802362323], [10.88888888888889, 8.396456087767515], [12.224256292906178, 8.63207032969941], [10.410334346504559, 5.951068168843178], [15.593333333333334, 10.933798770580863], [8.247669773635153, 5.9991048402484815], [15.694835680751174, 9.761067433833672], [11.585247883917775, 7.26046257734186], [18.131034482758622, 10.822871804759549], [13.88555078683834, 9.791447909499647], [13.251162790697675, 8.586749448410512], [18.996703296703295, 11.495483590432428], [16.936842105263157, 9.94145373214615], [13.666666666666666, 8.086930383589083], [12.658692185007974, 9.31916446682831]]\n",
      "Average sentence length English corpus: 13.85702144744974\n",
      "Average sentence length Korean corpus: 13.356887387565743\n"
     ]
    }
   ],
   "source": [
    "files = dir2files(dir_name, path = True, extension = '.txt')\n",
    "\n",
    "# index 0 is average sentence length for that story, index 1 is stdev of the sentence lengths for that story\n",
    "corpus_data_list_en = []\n",
    "corpus_data_list_ko = []\n",
    "\n",
    "for file in files:\n",
    "    if file[-8:].startswith('en'):\n",
    "        corpus_data_list_en.append(file_2_sentence_lengths(file, output_file_en))\n",
    "    if file[-8:].startswith('ko'):\n",
    "        corpus_data_list_ko.append(file_2_sentence_lengths(file, output_file_ko))\n",
    "\n",
    "en_lens = []\n",
    "ko_lens = []\n",
    "\n",
    "for story in corpus_data_list_en:\n",
    "    en_lens.append(story[0])\n",
    "for story in corpus_data_list_ko:\n",
    "    ko_lens.append(story[0])\n",
    "    \n",
    "avg_sen_len_en = float(sum(en_lens)) / float(len(en_lens))\n",
    "avg_sen_len_ko = float(sum(ko_lens)) / float(len(ko_lens))\n",
    "\n",
    "print(\"Average sentence length English corpus: \" + str(avg_sen_len_en))\n",
    "print(\"Average sentence length Korean corpus: \" + str(avg_sen_len_ko))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
